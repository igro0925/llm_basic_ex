{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e42ef36",
   "metadata": {},
   "source": [
    "# 허깅페이스 로그인 인증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee31c831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98104eefd88640fbb7be8a917e79d737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f363426",
   "metadata": {},
   "source": [
    "- 코드 안 나올 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a045bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login(new_session=True, write_permission=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b68409d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에 저장된 환경변수 로드\n",
    "load_dotenv(override=True)\n",
    "HF_READ_TOKEN = os.getenv(\"HF_READ_TOKEN\")\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# openai api 인증 및 openai 객체 생성\n",
    "client = OpenAI(api_key=HF_READ_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76605d5b",
   "metadata": {},
   "source": [
    "# 허깅페이스 transformers 라이브러리 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ab1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48774cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0274fd6a-0221-4de1-8071-f114820ff932)')' thrown while requesting HEAD https://huggingface.co/openai-community/gpt2/resolve/607a30d/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e1c3a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '인공지능이란 무엇인가?\\n\\n┌ 일리를 김이지능이란 무엇인가?\\n\\n┌ 일리를 김이지능이란 무엇인가?\\n\\n┌ 일리를 김이지능이란 무엇인가?\\n\\n┌ 일리를 김이지능이란 무엇인가?\\n\\n┌ 일리를 김이지능이란 무엇인가?\\n\\n┌ 일리를 김이지능이란 무엇인가?\\n\\n┌'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"인공지능이란 무엇인가?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2f899",
   "metadata": {},
   "source": [
    "=> text-generation 기본은 gpt2를 다운 받음\n",
    "=> gpt-2는 한국어를 잘 인식 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd80da6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'what is AI?\\n\\n\\nThe most important thing about AI is that it can be used to help people get better. In fact, it can be used to help people get better. As we will see, there are many different ways to achieve that goal. For example, AI can help people more effectively. It can help people to develop more skills and get better at their jobs. It can help people to be much more productive in their own lives. It can help people to be much more productive in their own lives. It can help people to be more productive in their own lives. It can help people to be much more productive in their own lives.\\n\\nIf you are looking for a way to build a better life for yourself, then AI can help you. It can help you to get better at how you live your life. It can help you to get better at your jobs. It can help you to be much more productive in your own lives. It can help people to be much more productive in their own lives. It can help people to be much more productive in their own lives. It can help people to be much more productive in their own lives. It can help people to be much more productive in their own lives.\\n\\nThe future of AI is going to be'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어 인식하기\n",
    "results = pipe(\"what is AI?\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a350be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is AI?\n",
      "\n",
      "\n",
      "The most important thing about AI is that it can be used to help people get better. In fact, it can be used to help people get better. As we will see, there are many different ways to achieve that goal. For example, AI can help people more effectively. It can help people to develop more skills and get better at their jobs. It can help people to be much more productive in their own lives. It can help people to be much more productive in their own lives. It can help people to be more productive in their own lives. It can help people to be much more productive in their own lives.\n",
      "\n",
      "If you are looking for a way to build a better life for yourself, then AI can help you. It can help you to get better at how you live your life. It can help you to get better at your jobs. It can help you to be much more productive in your own lives. It can help people to be much more productive in their own lives. It can help people to be much more productive in their own lives. It can help people to be much more productive in their own lives. It can help people to be much more productive in their own lives.\n",
      "\n",
      "The future of AI is going to be\n"
     ]
    }
   ],
   "source": [
    "print(results[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd076a0f",
   "metadata": {},
   "source": [
    "## 한국어를 학습한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bd530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Exception ignored in: <function tqdm.__del__ at 0x000001CE7B9C1E10>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "# task : text-generation\n",
    "# model : skt/kogpt2-base-v2\n",
    "txt_gen_ko = pipeline(\"text-generation\", model=\"skt/kogpt2-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d5ca2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '인공지능이란 무엇인가??\\n그것만이 인간의 능력을 넘어서는 것이다.\\n사람은 누구나 인간의 능력을 가지고 있다.\\n하지만 인간이 가지고 있는 능력은 지능과 다르다.\\n사람의 지능은 일반적으로 1분에 100번 정도 지능이 나온다.\\n하지만 인간의 능력은 1시간마다 100번 정도 지능이 된다.\\n그런데 사람의 지능은 1시간마다 100번, 150번 정도 지능이 나온다.\\n따라서 사람이 가지고 있는 능력은 1분에 100번 정도 지능이 난다.\\n그러면 인간은 어떻게 해야 인간의 능력을 뛰어넘을 수 있을까?\\n그 방법은 간단하다.\\n첫째, 지능이 없는 사람\\n일반적으로 인간은 지능이 1분에 100번 정도 지능이 있다.\\n그러나 인간은 4분에 100번 정도 지능이 있다.\\n즉 사람은 1분에 100번 정도 지능이 있다.\\n따라서 인간은 지능이 1분에 100번 정도 지능이 있다.\\n둘째, 지능이 1분에 100번 정도 지능이 있다.\\n즉 사람은 1분에 100번 정도 지능이 있다.\\n셋째, 지능이 1분에 100번 정도 지능이 있다.\\n이런 사람일수록 더 많은 일을 할 수 있다.\\n어떤 사람이 한 일에 100만원을 받는다고 하면 그는 100만원만 받을 수 있다.\\n그런데 보통 사람은 100만원만 받을 수 있다.\\n그런데 사람이 100만원을 받는다고 하면 그는 100만원만 받을 수 있다.\\n이런 사람일수록 한 일에 100만원씩만 받는다고 해도 그의 실력은 매우 우수할 것이다.\\n이런 사람이 많이'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_gen_ko(\"인공지능이란 무엇인가?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28fd832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_new_tokens: 256\n",
    "# do_sample: True\n",
    "# temperature: 0.7\n",
    "result = txt_gen_ko(\"토끼는\", do_sample=True, temperature=0.1, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b390e762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토끼는 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "이런 점에서 ‘사랑은 사랑’이라는 말은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는 뜻이다.\n",
      "사랑은 ‘사랑을 하는 사람’이라는\n"
     ]
    }
   ],
   "source": [
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3425b972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# LGAI-EXAONE/EXAONE-4.0-1.2B\n",
    "\n",
    "txt_gen_ko2 = pipeline(\"text-generation\", model=\"LGAI-EXAONE/EXAONE-4.0-1.2B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2180772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토끼는\n"
     ]
    }
   ],
   "source": [
    "result = txt_gen_ko2(\"토끼는\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb75084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|user|]\n",
      "너가 얼마나 대단한지 설명해 봐[|endofturn|]\n",
      "[|assistant|]\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "저는 EXAONE으로, LG AI Research에서 개발된 대규모 언어 모델입니다. 제 능력은 다음과 같은 점에서 뛰어납니다:\n",
      "\n",
      "1. **복잡한 계산 처리**: 다양한 언어 작업을 빠르고 정확하게 수행할 수 있습니다.\n",
      "2. **다양한 언어 이해 및 생성**: 한국어, 영어 등 여러 언어를 유창하게 이해하고 생성할 수 있습니다.\n",
      "3. **빠른 응답 속도**: 긴 텍스트도 짧은 시간 내에 분석하고 요약하거나 새로운 답변을 제공할 수 있습니다.\n",
      "4. **학습 데이터 활용**: 방대한\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    device_map=\"auto\" # \"cuda\" 로 설정 시 GPU 사용\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# choose your prompt\n",
    "prompt = \"Explain how wonderful you are\"\n",
    "prompt = \"Explica lo increíble que eres\"\n",
    "prompt = \"너가 얼마나 대단한지 설명해 봐\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids.to(model.device),\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855f7b5",
   "metadata": {},
   "source": [
    "## 감성 분석 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65df6c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "832b6aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9977974891662598}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classifier(\"I love using Huggin Face transformers\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fccae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 문장을 한거번에 분류 처리할때\n",
    "data = ['햄버거가 맛이 별로다', '나는 수영을 잘하지 못한다.', ' 이 카페의 커피맛이 예술이네.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "110f9212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.6179131865501404},\n",
       " {'label': 'POSITIVE', 'score': 0.7464603781700134},\n",
       " {'label': 'POSITIVE', 'score': 0.842933714389801}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = classifier(data)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "865934fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미션 - 아래와 같이 출력 되도록\n",
    "# This restaurant serves delicious food -> POSITIVE\n",
    "# I don't like the taste of this dish -> NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa19d59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998617172241211},\n",
       " {'label': 'NEGATIVE', 'score': 0.990011990070343}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"This restaurant serves delicious food\", \"I don't like the taste of this dish\"]\n",
    "results = classifier(data)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3439c1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This restaurant serves delicious food', {'label': 'POSITIVE', 'score': 0.9998617172241211})\n",
      "(\"I don't like the taste of this dish\", {'label': 'NEGATIVE', 'score': 0.990011990070343})\n"
     ]
    }
   ],
   "source": [
    "for zip_data in zip(data, results):\n",
    "    print(zip_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4eb16c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This restaurant serves delicious food -> POSITIVE\n",
      "I don't like the taste of this dish -> NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "for sen, ci in zip(data, results):\n",
    "    print(f\"{sen} -> {ci['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "962691be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This restaurant serves delicious food -> POSITIVE \n",
      "I don't like the taste of this dish -> NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "print(f\"{data[0]} -> {results[0]['label']} \\n{data[1]} -> {results[1]['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51dada",
   "metadata": {},
   "source": [
    "# 한국어 감성분석\n",
    "- 텍스트에서 긍정 또는 부정으로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff6c1f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.6179131865501404},\n",
       " {'label': 'POSITIVE', 'score': 0.7464603781700134},\n",
       " {'label': 'POSITIVE', 'score': 0.842933714389801}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['햄버거가 맛이 별로다', '나는 수영을 잘하지 못한다.', '이 카페의 커피맛이 예술이네.']\n",
    "result = classifier(data)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a0557",
   "metadata": {},
   "source": [
    "- 한국어 분류 가능한 모델을 사용해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502e127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Exception ignored in: <function tqdm.__del__ at 0x000001CE7B9C1E10>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# LABEL_0: negative\n",
    "# LABEL_1: positive\n",
    "\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model = \"WhitePeak/bert-base-cased-Korean-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be251f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['햄버거가 맛이 별로다', '나는 수영을 잘하지 못한다.', '이 카페의 커피맛이 예술이네.']\n",
    "result = sentiment_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d681a178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "햄버거가 맛이 별로다 -> negative\n",
      "나는 수영을 잘하지 못한다. -> negative\n",
      "이 카페의 커피맛이 예술이네. -> positive\n"
     ]
    }
   ],
   "source": [
    "# 문제\n",
    "# 햄버거가 맛이 별로다  => NEGATIVE\n",
    "# 나는 수영을 잘하지 못한다.  => NEGATIVE\n",
    "# 이 카페의 커피맛이 예술이네. => POSITIVE\n",
    "\n",
    "for sen, ci in zip(data, result):\n",
    "\n",
    "    label = \"positive\" if ci['label'] == 'LABEL_1' else \"negative\"\n",
    "    print(f\"{sen.strip()} -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57862a",
   "metadata": {},
   "source": [
    "## 좋은 모델 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "890287e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca009c5a35f4184ba1ad73c9928189f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--google--gemma-3-270m-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc64c93b650c4a53a99b902e36a01214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fe80f5882144cea10a76b99fe73ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07461c253acb48069a2d885a7cd0de22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd14d5743474a81a1af576609aa9e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d7ab02650f49e59de1c1836a9559af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d350d3fcda44e7fbe871063b7e39cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c796b45b91fe4dd8b0d562622bf7cec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c6324c9a8142b0b09a3919436e533b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# google/gemma-3-270m-it, 승인 필요\n",
    "generation = pipeline(\"text-generation\", model=\"google/gemma-3-270m-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be7d848f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인공지능이란?\\n\\n인공지능은 인간의 통제를 벗어나, 스스로 학습하고 발전하는 컴퓨터 시스템입니다.\\n\\n인공지능은 다양한 분야에서 활용될 수 있으며, 특히 기술, 경제, 사회 등 다양한 분야에서 혁신적인 변화를 가져올 것으로 기대됩니다.\\n\\n인공지능은 현재의 기술 수준으로는 거의 불가능한 기술이지만, 앞으로 더 많은 사람들이 인공지능을 활용할 수 있게 될 것이라는 전망이 있습니다.\\n\\n**인공지능의 종류**\\n\\n인공지능은 다양한 종류가 있으며, 그 종류는 다음과 같습니다.\\n\\n*   **컴퓨터 인공지능 (AI):** 컴퓨터 시스템의 작동 원리를 이해하고, 인간의 지능을 활용하는 것을 목표로 합니다.\\n*   **이미지 인식 AI (Image Recognition AI):** 이미지와 데이터를 분석하여 이미지를 인식하고, 이를 통해 이미지를 이해하고 분류하는 기술입니다.\\n*   **음성 인식 AI (Speech Recognition AI):** 음성 데이터를 분석하여 음성을 인식하고, 이를 통해 음성을 이해하고 분류하는 기술입니다.\\n*   **반응형 AI (Reactive AI):** 특정 상황에 따라 즉각적으로 반응하는 기술입니다. 예를 들어, 스마트'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation(\"인공지능이란?\")[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654040c6",
   "metadata": {},
   "source": [
    "## 문장 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6696837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000001CE7B9C1E10>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "summ = pipeline('summarization', framework=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e20f8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test= \"\"\"\n",
    "임영웅의 출연으로 화제를 모은 '불후의 명곡'에 '테크노 여전사' '원조 멀티테이너'로 불리는 이정현이 아티스트로 출격한다.\n",
    "\n",
    "12일 아이즈(IZE) 취재 결과, 오는 22일 진행되는 KBS 2TV '불후의 명곡'(이하 '불후') 녹화가 '아티스트 이정현 편'으로 꾸며진다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0024d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"  \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0pronounced  \\xa0- \\xa0\\xa0-\\xa0- 'И� \\xa0' \\xa0KBS 2TV '불후의   테크   '텬 \\xa0’  ’�’ – \\xa0 '명’ - is a Korean TV show hosted by KBS 2 TV .\"}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ(sample_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db8694",
   "metadata": {},
   "source": [
    "- 한글 인식이 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bbbb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eabc1f098264aecbc77035fb14a0cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\llm_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--psyche--KoT5-summarization. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cf52dc5286476bb4995207e9937e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# psyche/KoT5-summarization\n",
    "summ = pipeline('summarization', model=\"psyche/KoT5-summarization\", framework=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea9ff1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
